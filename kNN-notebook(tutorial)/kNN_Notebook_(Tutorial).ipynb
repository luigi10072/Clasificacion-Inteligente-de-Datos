{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1.1 Fundamentos de la técnica\n",
        "\n",
        "El algoritmo **k-Nearest Neighbors (kNN)** es uno de los métodos de clasificación más simples y efectivos.\n",
        "Su principio básico es que los objetos similares tienden a estar cerca en el espacio de características.\n",
        "\n",
        "Dado un nuevo punto de datos, el algoritmo:\n",
        "\n",
        "1. Calcula la distancia entre el punto y todos los ejemplos del conjunto de\n",
        "entrenamiento.\n",
        "\n",
        "2. Selecciona los k vecinos más cercanos.\n",
        "\n",
        "3. Asigna la clase más frecuente entre esos vecinos al nuevo punto.\n",
        "\n",
        "Es un modelo no paramétrico, lo que significa que no aprende una función explícita durante el entrenamiento; en cambio, “memoriza” los datos y toma decisiones en el momento de la predicción."
      ],
      "metadata": {
        "id": "lHdlVh9sq3H-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2 Modelo Matemático\n",
        "\n",
        "El modelo se basa en una medida de distancia, comúnmente la **distancia euclidiana**:\n",
        "$$ d(x,x_i)=\\sqrt{\\sum_{j=1}^n(x_j−x_{ij})2} $$\n",
        "\n",
        "Luego, para clasificar un nuevo ejemplo x:\n",
        "$$\\hat y=mode\\left\\{yi:xi∈Nk(x)\\right\\} $$\n",
        "\n",
        "donde:\n",
        "* $N_K(x)$: conjunto de los k vecinos más cercanos a x\n",
        "* $\\hat y$: clase predicha"
      ],
      "metadata": {
        "id": "JIhROlE_rEH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3 Librerías, clases y funciones utilizadas\n",
        "|Librería\t|Función / Clase|\tDescripción|\n",
        "|-|-|-|\n",
        "|pandas|\tDataFrame|\tManipulación de datos tabulares|\n",
        "|matplotlib.pyplot / seaborn\t|scatterplot, pairplot\t|Visualización de datos|\n",
        "|sklearn.datasets\t|load_iris\t|Carga el dataset de ejemplo Iris|\n",
        "|sklearn.model_selection|\ttrain_test_split|\tDivide los datos en conjuntos de entrenamiento y prueba|\n",
        "|sklearn.preprocessing\t|StandardScaler\t|Estandariza los datos (media = 0, varianza = 1)|\n",
        "|sklearn.neighbors|\tKNeighborsClassifier|\tImplementa el algoritmo kNN|\n",
        "|sklearn.metrics|\tconfusion_matrix, accuracy_score\t|Evalúan el rendimiento del modelo|"
      ],
      "metadata": {
        "id": "COgrwMblt20E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.4 Pipeline\n",
        "**1.4.1 Preprocesamiento de datos**"
      ],
      "metadata": {
        "id": "gGJSchmTuYXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar librerías\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Cargar el dataset\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = pd.Series(iris.target, name='target')\n",
        "\n",
        "# Análisis estadístico\n",
        "print(\"Resumen estadístico del dataset:\")\n",
        "display(X.describe())\n",
        "\n",
        "# Añadir la columna de clase\n",
        "df = X.copy()\n",
        "df['target'] = y\n",
        "\n",
        "# Gráfico de dispersión\n",
        "sns.pairplot(df, hue='target', palette='Set2')\n",
        "plt.suptitle(\"Gráfico de dispersión del dataset Iris\", y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Pgzo9Ygdq2ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.4.2 Feature Engineering**\n",
        "\n",
        "Seleccionamos las variables predictoras (```sepal length```, ```sepal width```, ```petal length```, ```petal width```) y la variable objetivo (```target```), que representa el tipo de flor (Setosa, Versicolor o Virginica).\n",
        "\n",
        "Las variables se escalan para evitar que las diferencias de magnitud afecten la distancia euclidiana."
      ],
      "metadata": {
        "id": "qN6D6Kvmuf40"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7-Sh7-jqWl1"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Dividir datos\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Escalado de características\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.4.3 Selección del Modelo**\n",
        "\n",
        "El modelo kNN se selecciona porque:\n",
        "\n",
        "* Es intuitivo y fácil de implementar.\n",
        "\n",
        "* No requiere entrenamiento intensivo.\n",
        "\n",
        "* Es ideal para conjuntos de datos pequeños o con fronteras de decisión no lineales."
      ],
      "metadata": {
        "id": "pRye7Li6vPAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Crear modelo kNN con k = 5\n",
        "knn = KNeighborsClassifier(n_neighbors=5)"
      ],
      "metadata": {
        "id": "Y_j7YvsQvYuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.4.4 Entrenamiento del Modelo**"
      ],
      "metadata": {
        "id": "bjO1OigXvZyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el modelo\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "print(\"Modelo entrenado con éxito.\")"
      ],
      "metadata": {
        "id": "3BSKh9IWvd4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.4.5 Predicción**\n",
        "\n",
        "Se crea una función para predecir la clase de un nuevo patrón."
      ],
      "metadata": {
        "id": "MkTH-IdUvhW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predecir_flor(nuevo_patron):\n",
        "    \"\"\"\n",
        "    Recibe una lista con las 4 medidas de una flor\n",
        "    y devuelve la clase predicha.\n",
        "    \"\"\"\n",
        "    patron_escalado = scaler.transform([nuevo_patron])\n",
        "    pred = knn.predict(patron_escalado)\n",
        "    return iris.target_names[pred[0]]\n",
        "\n",
        "# Ejemplo de prueba\n",
        "nuevo = [5.8, 3.1, 5.0, 1.7]\n",
        "print(f\"Predicción para {nuevo}: {predecir_flor(nuevo)}\")"
      ],
      "metadata": {
        "id": "8-If3vmpvg_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.4.6 Evaluación del Modelo**\n",
        "\n",
        "Evaluamos el modelo con Accuracy y la Matriz de Confusión."
      ],
      "metadata": {
        "id": "qYw8VVi-voyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
        "\n",
        "# Predicciones sobre el conjunto de prueba\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "\n",
        "# Cálculo de métricas\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "matriz = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy del modelo: {acc:.2f}\")\n",
        "\n",
        "# Mostrar matriz de confusión\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=matriz, display_labels=iris.target_names)\n",
        "disp.plot(cmap=\"Greens\")\n",
        "plt.title(\"Matriz de Confusión del Modelo kNN\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vuXK2__0vqSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.4.7 Ajuste de Parámetro k**\n",
        "\n",
        "Para elegir el valor óptimo de k, probamos varios valores y observamos el error de clasificación."
      ],
      "metadata": {
        "id": "zkk3fuC0vsGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "errors = []\n",
        "k_values = range(1, 21)\n",
        "\n",
        "for k in k_values:\n",
        "    model = KNeighborsClassifier(n_neighbors=k)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    pred_k = model.predict(X_test_scaled)\n",
        "    errors.append(1 - accuracy_score(y_test, pred_k))\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(k_values, errors, marker='o', linestyle='dashed')\n",
        "plt.title(\"Selección del parámetro k óptimo\")\n",
        "plt.xlabel(\"Valor de k\")\n",
        "plt.ylabel(\"Error de Clasificación\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_7CuMt4Hvv_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusión**\n",
        "\n",
        "El clasificador kNN demostró ser altamente preciso con el conjunto de datos Iris.\n",
        "Su simplicidad lo hace ideal para tareas de clasificación iniciales o como modelo base de comparación.\n",
        "El valor de $k$ debe elegirse cuidadosamente, equilibrando el sesgo y la varianza.\n",
        "\n",
        "\n",
        "---\n",
        "\\\n",
        "**Referencias Bibliográficas**\n",
        "\n",
        "* Géron, A. (2019). Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow. O’Reilly Media.\n",
        "\n",
        "* Scikit-learn Documentation. https://scikit-learn.org/stable/\n",
        "\n",
        "* Han, J., Kamber, M., & Pei, J. (2011). Data Mining: Concepts and Techniques. Morgan Kaufmann.\n",
        "\n",
        "* Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer."
      ],
      "metadata": {
        "id": "1x2hY86AvyHM"
      }
    }
  ]
}